# -*- coding: utf-8 -*-
"""MESO:Telecom Customer Churn Prediction using PySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_HgePwV5MGIjFwz7wrJ_R0UGHazixHkV

# **Background Information**
Customer churn is a significant challenge in the telecom industry. Identifying customers who are
likely to churn is crucial for implementing proactive measures to retain them. By leveraging PySpark,
we can take advantage of its distributed computing capabilities to handle large volumes of data
efficiently and build an accurate machine learning model for churn prediction.

# **Problem Statement**
The goal of this project is to develop a machine learning model using PySpark that accurately
predicts customer churn in a telecom company. The model should achieve a minimum accuracy of
0.8, enabling the company to proactively identify and retain customers at risk of leaving. By
effectively predicting churn, the company can implement targeted retention strategies, reduce
customer attrition, and improve overall business performance.
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier, LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.classification import LogisticRegression, GBTClassifier, LinearSVC
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import col
import pandas as pd
import io

from google.colab import files
files.upload()

# Create a Spark session
spark = SparkSession.builder \
    .appName("data_churn") \
    .config("spark.logConf", "false")\
    .getOrCreate()

data = spark.read.csv('telecom_dataset.csv',inferSchema=True, header=True)

data.head(10)

data.dtypes

#DATA_ preprocessing

# Convert Age column to integer type
df_updated = data.withColumn("Age", col("Age").cast("integer"))

# Convert MonthlyCharges and TotalCharges column to float type
df_updated = df_updated.withColumn("MonthlyCharges", col("MonthlyCharges").cast("float"))
df_updated  = df_updated.withColumn("TotalCharges", col("TotalCharges").cast("float"))

# Calculating call duration (assuming call_start_time and call_end_time columns are present)
df_updated  = df_updated.withColumn("call_duration", (col("TotalCharges") - col("MonthlyCharges")) / 60)

# Calculating average monthly spend
df_updated = df_updated.withColumn("average_monthly_spend", col("MonthlyCharges"))

df_updated.printSchema()

# Encodinging categorical variables
categorical_cols = ['Contract', 'Churn','Gender']
indexers = [StringIndexer(inputCol=col, outputCol=col+'_index').fit(df_updated) for col in categorical_cols]
pipeline = Pipeline(stages=indexers)
df_updated = pipeline.fit(df_updated).transform(df_updated)
# Feature scaling
assembler = VectorAssembler(inputCols=['Age', 'average_monthly_spend', 'call_duration', 'Gender_index'], outputCol='features')
df_updated = assembler.transform(df_updated)

scaler = StandardScaler(inputCol='features', outputCol='scaled_features')
scaler_model = scaler.fit(df_updated)
df_updated = scaler_model.transform(df_updated)

# Splitting the data into training and testing sets
train_data, test_data = df_updated.randomSplit([0.7, 0.3], seed=34)

#Model training and evaluation
lr = LogisticRegression(labelCol='Churn_index', featuresCol='scaled_features')

#

#Model selection and training
classifiers = [
    LogisticRegression(labelCol='Churn_index', featuresCol='scaled_features'),
    RandomForestClassifier(labelCol='Churn_index', featuresCol='scaled_features'),
    GBTClassifier(labelCol='Churn_index', featuresCol='scaled_features'),
    LinearSVC(labelCol='Churn_index', featuresCol='scaled_features')
]

# Defining the parameter grid for each classifier
paramGrids = [
    ParamGridBuilder()
        .addGrid(LogisticRegression.regParam, [0.1, 0.01])
        .addGrid(LogisticRegression.elasticNetParam, [0.0, 0.5, 1.0])
        .build(),
    ParamGridBuilder()
        .addGrid(RandomForestClassifier.numTrees, [10, 20, 30])
        .addGrid(RandomForestClassifier.featureSubsetStrategy, ['auto', 'sqrt'])
        .build(),
    ParamGridBuilder()
        .addGrid(GBTClassifier.maxDepth, [5, 10])
        .addGrid(GBTClassifier.maxIter, [20, 30])
        .build(),
    ParamGridBuilder()
        .addGrid(LinearSVC.maxIter, [10, 20])
        .addGrid(LinearSVC.regParam, [0.1, 0.01])
        .build()
]

evaluator = BinaryClassificationEvaluator(labelCol='Churn_index')

best_model = None
best_accuracy = 0.0

# Iterating over classifiers and parameter grids
for classifier, paramGrid in zip(classifiers, paramGrids):
    pipeline = Pipeline(stages=[classifier])
    crossval = CrossValidator(estimator=pipeline,
                              estimatorParamMaps=paramGrid,
                              evaluator=evaluator,
                              numFolds=5)
    cv_model = crossval.fit(train_data)

# Model evaluation on test data
predictions = cv_model.transform(test_data)
accuracy = evaluator.evaluate(predictions)

print(f"Accuracy for {classifier.__class__.__name__}: {accuracy}")

if accuracy > best_accuracy:
        best_model = cv_model.bestModel
        best_accuracy = accuracy



# Creating a MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol='Churn_index', predictionCol='prediction')

# Evaluating the best model on test data
accuracy = evaluator.evaluate(best_predictions, {evaluator.metricName: 'accuracy'})
f1_score = evaluator.evaluate(best_predictions, {evaluator.metricName: 'f1'})

print(f"Accuracy: {accuracy}")
print(f"F1-Score: {f1_score}")



"""# **Results: **
The accuracy of the trained model on the testing data was computed as 0.6.
"""